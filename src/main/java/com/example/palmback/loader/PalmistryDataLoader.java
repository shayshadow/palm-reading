package com.example.palmback.loader;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.ai.document.Document;
import org.springframework.ai.embedding.EmbeddingModel;
import org.springframework.ai.transformer.splitter.TokenTextSplitter;
import org.springframework.ai.vectorstore.VectorStore;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.CommandLineRunner;
import org.springframework.core.io.Resource;
import org.springframework.stereotype.Component;

import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

@Component
public class PalmistryDataLoader implements CommandLineRunner {

    private static final Logger log = LoggerFactory.getLogger(PalmistryDataLoader.class);
    private final VectorStore vectorStore;
    private final EmbeddingModel embeddingModel;

    @Value("classpath:palm-data/part1.txt") // í´ë” êµ¬ì¡°ì— ë§ê²Œ ê²½ë¡œ í™•ì¸!
    private Resource part1;

    @Value("classpath:palm-data/part2.txt")
    private Resource part2;

    @Value("classpath:palm-data/part3.txt")
    private Resource part3;

    public PalmistryDataLoader(VectorStore vectorStore,EmbeddingModel embeddingModel) {
        this.vectorStore = vectorStore;
        this.embeddingModel = embeddingModel;
    }

    @Override
    public void run(String... args) {
        log.info("ğŸ”® The Digital Mystic: ê³ ëŒ€ ì§€ì‹ì„ ì²œì²œíˆ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...");

        try {
            List<Document> allDocuments = new ArrayList<>();

            // 1. ëª¨ë“  ë°ì´í„° ì¼ë‹¨ ë©”ëª¨ë¦¬ì— ë¡œë“œ
            allDocuments.addAll(loadAndChunk(part1, "Part 1 - Introduction & Shape"));
            allDocuments.addAll(loadAndChunk(part2, "Part 2 - The Mounts"));
            allDocuments.addAll(loadAndChunk(part3, "Part 3 - The Lines"));

            if (allDocuments.isEmpty()) {
                log.warn("âš ï¸ ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.");
                return;
            }

            // âœ… ì„ë² ë”© ì°¨ì› ì‚¬ì „ ì²´í¬: ì²« ì¡°ê° í•œ ê°œë§Œ ì„ë² ë”©í•´ì„œ ì°¨ì› í™•ì¸
            Document d0 = allDocuments.get(0);
            int dims = embeddingModel.embed(d0).length;
            log.info("âœ… embedding dims = {}", dims);

            // âœ… HNSW + pgvector(vector) ì œí•œ(<=2000) ìœ„ë°˜ì´ë©´ ì—¬ê¸°ì„œ ì¦‰ì‹œ ì¤‘ë‹¨
            if (dims > 2000) {
                throw new IllegalStateException(
                        "Embedding dimension is " + dims +
                                " (>2000). pgvector HNSW index cannot be created for vector type. " +
                                "Fix embedding dimensionality (e.g., 768/1536) or change storage type.");
            }

            log.info("ì´ {}ê°œì˜ ì§€ì‹ ì¡°ê°ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. API ì œí•œì„ í”¼í•´ ë‚˜ëˆ ì„œ ì €ì¥í•©ë‹ˆë‹¤.", allDocuments.size());

            // 2. [Rate Limit íšŒí”¼] ë°°ì¹˜ ì²˜ë¦¬ (10ê°œì”© ëŠì–´ì„œ ì €ì¥ + íœ´ì‹)
            int batchSize = 10; // í•œ ë²ˆì— ë³´ë‚¼ ê°œìˆ˜ (ì•ˆì „í•˜ê²Œ 10ê°œ)
            for (int i = 0; i < allDocuments.size(); i += batchSize) {
                int end = Math.min(i + batchSize, allDocuments.size());
                List<Document> batch = allDocuments.subList(i, end);

                try {

                    List<Document> safeBatch = batch.stream()
                            .filter(d -> d.getText() != null && !d.getText().trim().isBlank())
                            .filter(d -> d.getText().trim().length() >= 10)
                            .toList();

                    if (safeBatch.isEmpty()) {
                        log.warn("âš ï¸ ì´ë²ˆ ë°°ì¹˜ëŠ” ëª¨ë‘ ë¹ˆ/ì§§ì€ ì²­í¬ë¼ ìŠ¤í‚µí•©ë‹ˆë‹¤. ({}~{})", i, end);
                        continue;
                    }
                    vectorStore.add(safeBatch);
                    log.info("âœ… ì €ì¥ ì¤‘... ({}/{})", end, allDocuments.size());

                    // ğŸš¨ ì¤‘ìš”: êµ¬ê¸€ API í˜•ë‹˜ì´ í™”ë‚´ì§€ ì•Šê²Œ 2ì´ˆ ì‰½ë‹ˆë‹¤.
                    Thread.sleep(2000);

                } catch (Exception e) {
                    log.error("âŒ ë°°ì¹˜ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰): ", e);
                    // 429ê°€ ë˜ ëœ¨ë©´ ì¡°ê¸ˆ ë” ì˜¤ë˜ ì‰¬ê²Œ ì„¤ì •
                    Thread.sleep(5000);
                }
            }

            log.info("âœ¨ ëª¨ë“  ì§€ì‹ ì£¼ì… ì™„ë£Œ! ì´ì œ ì ì„ ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ”®");

        } catch (Exception e) {
            log.error("âŒ ì§€ì‹ ë¡œë”© ì¹˜ëª…ì  ì˜¤ë¥˜: ", e);
        }
    }

    private List<Document> loadAndChunk(Resource resource, String sourceTitle) throws IOException {
        if (!resource.exists()) return List.of();

        String content = new String(resource.getContentAsByteArray(), StandardCharsets.UTF_8);
        Map<String, Object> metadata = Map.of("source", sourceTitle, "category", "Scientific Palmistry");
        Document rawDoc = new Document(content, metadata);

        // ì²­í‚¹ ì„¤ì • (ì•„ê¹Œë‘ ë™ì¼)
        TokenTextSplitter splitter = TokenTextSplitter.builder()
                .withChunkSize(1000)
                .withMinChunkSizeChars(200)
                .withMinChunkLengthToEmbed(10)
                .withMaxNumChunks(10000)
                .withKeepSeparator(true)
                .build();

        List<Document> chunks = splitter.apply(List.of(rawDoc));

        // âœ… í•µì‹¬: ê³µë°±/ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œê±° (ì´ê±° ì—†ìœ¼ë©´ embeddings=0 ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
        return chunks.stream()
                .filter(d -> d.getText() != null)
                .map(d -> new Document(d.getText().trim(), d.getMetadata()))
                .filter(d -> !d.getText().isBlank())
                .filter(d -> d.getText().length() >= 10)  // ì•ˆì „ì¥ì¹˜(ì›í•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œ ì¡°ì •)
                .toList();
    }
}